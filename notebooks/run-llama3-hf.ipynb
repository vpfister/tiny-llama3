{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0845e4641e58c0",
   "metadata": {},
   "source": [
    "# finetuning llama3 locally\n",
    "\n",
    "foolow this [datacamp blog post](https://www.datacamp.com/tutorial/llama3-fine-tuning-locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:20:26.272078Z",
     "start_time": "2024-06-27T17:20:24.164158Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments, \n",
    "    pipeline, \n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os\n",
    "import torch\n",
    "import wandb \n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "import dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9cb6b27bb7c4e5",
   "metadata": {},
   "source": [
    "## Fine-tuning Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cc38149631065",
   "metadata": {},
   "source": [
    "### setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e926bb061b678d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:20:26.276277Z",
     "start_time": "2024-06-27T17:20:26.272778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincent-pfister\u001b[0m (\u001b[33mraisepartner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vincent/dev/ai/tiny-llama3/notebooks/wandb/run-20240628_115657-17p3dotz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raisepartner/llama3-fine-tune-tutorial/runs/17p3dotz' target=\"_blank\">ancient-meadow-9</a></strong> to <a href='https://wandb.ai/raisepartner/llama3-fine-tune-tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raisepartner/llama3-fine-tune-tutorial' target=\"_blank\">https://wandb.ai/raisepartner/llama3-fine-tune-tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raisepartner/llama3-fine-tune-tutorial/runs/17p3dotz' target=\"_blank\">https://wandb.ai/raisepartner/llama3-fine-tune-tutorial/runs/17p3dotz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dotenv.load_dotenv()\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "wandb_api_key = os.environ[\"WANDB_API_KEY\"]\n",
    "# would log in with (not necessary if already logged in): wandb.login(key=wandb_api_key)\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"llama3-fine-tune-tutorial\",\n",
    "    job_type=\"train\",\n",
    "    anonymous=\"allow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76dfc44e4a63d33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:26:43.607398Z",
     "start_time": "2024-06-27T17:26:43.604874Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "new_model = \"llama-3-8B-chat-doctor\"\n",
    "torch_dtype = torch.float16\n",
    "attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5b60b2fa31ebe",
   "metadata": {},
   "source": [
    "### loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6092787b08b104d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:36:01.400731Z",
     "start_time": "2024-06-27T17:30:51.595670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda5c4a8a704b429e9d07afda5d9d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# qlora config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbd3ca0cbcafad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T17:58:36.333711Z",
     "start_time": "2024-06-27T17:58:34.429274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a037e1c8532bd",
   "metadata": {},
   "source": [
    "### adding the adapter to the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd584102b26c77a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:04:31.388446Z",
     "start_time": "2024-06-27T18:04:30.948267Z"
    }
   },
   "outputs": [],
   "source": [
    "# lora config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"k_proj\",\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d06a8964175db00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:18:21.251095Z",
     "start_time": "2024-06-27T18:18:14.883877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|im_end|>\\n<|im_start|>assistant\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|im_end|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(10000)) # Only use 1000 samples for quick demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438606764b5f8973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:18:28.609017Z",
     "start_time": "2024-06-27T18:18:28.580205Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c89d70cb6fa91f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:19:04.503916Z",
     "start_time": "2024-06-27T18:19:04.490933Z"
    }
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.002,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eceb74f0d7d9c67d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T18:19:59.959586Z",
     "start_time": "2024-06-27T18:19:06.598173Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/transformers/training_args.py:1961: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07feade1b7ae4b4d9b939ea67f73478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c02c1b33cf4f838a584a580dcf6850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf8690528ac66cf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T19:55:56.571337Z",
     "start_time": "2024-06-27T18:20:05.929704Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 9:58:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.834383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.008100</td>\n",
       "      <td>2.641920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.616600</td>\n",
       "      <td>2.618799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.597500</td>\n",
       "      <td>2.600470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.578700</td>\n",
       "      <td>2.591217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.494200</td>\n",
       "      <td>2.584454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.527700</td>\n",
       "      <td>2.569757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.577900</td>\n",
       "      <td>2.558612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.545100</td>\n",
       "      <td>2.566888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.602600</td>\n",
       "      <td>2.555167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.602600</td>\n",
       "      <td>2.574419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>2.721300</td>\n",
       "      <td>2.557128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.590700</td>\n",
       "      <td>2.543875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>2.614200</td>\n",
       "      <td>2.537050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.579900</td>\n",
       "      <td>2.545599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.552200</td>\n",
       "      <td>2.535331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.619700</td>\n",
       "      <td>2.534675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2.505000</td>\n",
       "      <td>2.524899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>2.556200</td>\n",
       "      <td>2.515827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.508400</td>\n",
       "      <td>2.521979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>2.508400</td>\n",
       "      <td>2.515591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>2.547600</td>\n",
       "      <td>2.523139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>2.442500</td>\n",
       "      <td>2.522091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>2.592400</td>\n",
       "      <td>2.517969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.665300</td>\n",
       "      <td>2.510085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>2.503293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.541000</td>\n",
       "      <td>2.504192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>2.485200</td>\n",
       "      <td>2.521638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>2.480600</td>\n",
       "      <td>2.501933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.456100</td>\n",
       "      <td>2.491735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>2.456100</td>\n",
       "      <td>2.499635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>2.656200</td>\n",
       "      <td>2.497881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>2.380300</td>\n",
       "      <td>2.505922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>2.606200</td>\n",
       "      <td>2.488451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>2.564500</td>\n",
       "      <td>2.482102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>2.618500</td>\n",
       "      <td>2.477846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>2.303900</td>\n",
       "      <td>2.480267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>2.444300</td>\n",
       "      <td>2.485436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>2.537200</td>\n",
       "      <td>2.488493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.580700</td>\n",
       "      <td>2.483707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>2.580700</td>\n",
       "      <td>2.474839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>2.347600</td>\n",
       "      <td>2.473250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>2.462700</td>\n",
       "      <td>2.472576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>2.401100</td>\n",
       "      <td>2.468663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>2.509400</td>\n",
       "      <td>2.465898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>2.407500</td>\n",
       "      <td>2.465677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>2.340300</td>\n",
       "      <td>2.459424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>2.551700</td>\n",
       "      <td>2.463953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>2.375400</td>\n",
       "      <td>2.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.408400</td>\n",
       "      <td>2.457099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>2.408400</td>\n",
       "      <td>2.459035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>2.417000</td>\n",
       "      <td>2.453437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>2.422200</td>\n",
       "      <td>2.450072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>2.397900</td>\n",
       "      <td>2.452055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>2.338400</td>\n",
       "      <td>2.457634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>2.659900</td>\n",
       "      <td>2.455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>2.450900</td>\n",
       "      <td>2.455820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>2.549100</td>\n",
       "      <td>2.442343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>2.346200</td>\n",
       "      <td>2.439236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.540500</td>\n",
       "      <td>2.443585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>2.540500</td>\n",
       "      <td>2.454430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>2.355600</td>\n",
       "      <td>2.446375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>2.430400</td>\n",
       "      <td>2.444001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>2.513000</td>\n",
       "      <td>2.438334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>2.441200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>2.402300</td>\n",
       "      <td>2.443398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>2.215300</td>\n",
       "      <td>2.444924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>2.284500</td>\n",
       "      <td>2.439165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>2.332700</td>\n",
       "      <td>2.436524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.264300</td>\n",
       "      <td>2.441833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>2.264300</td>\n",
       "      <td>2.442660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>2.287600</td>\n",
       "      <td>2.449475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>2.598200</td>\n",
       "      <td>2.436348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>2.556500</td>\n",
       "      <td>2.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>2.478300</td>\n",
       "      <td>2.433142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.510800</td>\n",
       "      <td>2.431739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>2.455800</td>\n",
       "      <td>2.428931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>2.545900</td>\n",
       "      <td>2.433667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>2.210400</td>\n",
       "      <td>2.432535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.591100</td>\n",
       "      <td>2.429078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>2.591100</td>\n",
       "      <td>2.427475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>2.478000</td>\n",
       "      <td>2.435073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>2.441600</td>\n",
       "      <td>2.432686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>2.351300</td>\n",
       "      <td>2.425010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>2.402200</td>\n",
       "      <td>2.421014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>2.498800</td>\n",
       "      <td>2.417142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>2.449300</td>\n",
       "      <td>2.421630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>2.485700</td>\n",
       "      <td>2.419764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>2.625800</td>\n",
       "      <td>2.425033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.397800</td>\n",
       "      <td>2.415218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>2.397800</td>\n",
       "      <td>2.409885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>2.436600</td>\n",
       "      <td>2.405914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>2.420900</td>\n",
       "      <td>2.408867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>2.577300</td>\n",
       "      <td>2.418862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>2.675100</td>\n",
       "      <td>2.413864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>2.330700</td>\n",
       "      <td>2.412016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>2.210800</td>\n",
       "      <td>2.409239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>2.406400</td>\n",
       "      <td>2.413536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>2.123500</td>\n",
       "      <td>2.407538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>2.410799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>2.265000</td>\n",
       "      <td>2.409379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>2.494700</td>\n",
       "      <td>2.406219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>1.989500</td>\n",
       "      <td>2.399085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>2.509800</td>\n",
       "      <td>2.403505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>2.183200</td>\n",
       "      <td>2.403536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>2.535100</td>\n",
       "      <td>2.408395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>2.044300</td>\n",
       "      <td>2.401424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>2.470600</td>\n",
       "      <td>2.400840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>2.451300</td>\n",
       "      <td>2.397923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.320000</td>\n",
       "      <td>2.406106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>2.320000</td>\n",
       "      <td>2.402585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>2.519000</td>\n",
       "      <td>2.395896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>2.338900</td>\n",
       "      <td>2.393312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>2.339700</td>\n",
       "      <td>2.393556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>2.429300</td>\n",
       "      <td>2.394915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>2.441200</td>\n",
       "      <td>2.399910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>2.513500</td>\n",
       "      <td>2.402505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>2.504500</td>\n",
       "      <td>2.398307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>2.288600</td>\n",
       "      <td>2.394625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.377100</td>\n",
       "      <td>2.388522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>2.377100</td>\n",
       "      <td>2.390769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1098</td>\n",
       "      <td>2.357100</td>\n",
       "      <td>2.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>2.590100</td>\n",
       "      <td>2.388351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>2.563800</td>\n",
       "      <td>2.386067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>2.435500</td>\n",
       "      <td>2.380554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>2.492300</td>\n",
       "      <td>2.385749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1143</td>\n",
       "      <td>2.569500</td>\n",
       "      <td>2.388842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>2.586800</td>\n",
       "      <td>2.391083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>2.269900</td>\n",
       "      <td>2.386692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.402900</td>\n",
       "      <td>2.387676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1179</td>\n",
       "      <td>2.402900</td>\n",
       "      <td>2.385573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1188</td>\n",
       "      <td>2.561800</td>\n",
       "      <td>2.381205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1197</td>\n",
       "      <td>2.412900</td>\n",
       "      <td>2.380397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>2.441900</td>\n",
       "      <td>2.377705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>2.409100</td>\n",
       "      <td>2.374107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>2.304200</td>\n",
       "      <td>2.377949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1233</td>\n",
       "      <td>2.303600</td>\n",
       "      <td>2.379411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>2.255400</td>\n",
       "      <td>2.383947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1251</td>\n",
       "      <td>2.532600</td>\n",
       "      <td>2.389560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.463800</td>\n",
       "      <td>2.387769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1269</td>\n",
       "      <td>2.463800</td>\n",
       "      <td>2.380010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>2.334100</td>\n",
       "      <td>2.372993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287</td>\n",
       "      <td>2.417900</td>\n",
       "      <td>2.377423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>2.026800</td>\n",
       "      <td>2.375677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>2.399900</td>\n",
       "      <td>2.377014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1314</td>\n",
       "      <td>2.340700</td>\n",
       "      <td>2.375706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1323</td>\n",
       "      <td>2.597500</td>\n",
       "      <td>2.373014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1332</td>\n",
       "      <td>2.279300</td>\n",
       "      <td>2.374661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1341</td>\n",
       "      <td>2.235600</td>\n",
       "      <td>2.371716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.409800</td>\n",
       "      <td>2.375436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1359</td>\n",
       "      <td>2.409800</td>\n",
       "      <td>2.372020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>2.173900</td>\n",
       "      <td>2.375121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1377</td>\n",
       "      <td>2.286800</td>\n",
       "      <td>2.367291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1386</td>\n",
       "      <td>2.490800</td>\n",
       "      <td>2.367821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>2.138100</td>\n",
       "      <td>2.372787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>2.674000</td>\n",
       "      <td>2.372119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1413</td>\n",
       "      <td>2.404600</td>\n",
       "      <td>2.367586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1422</td>\n",
       "      <td>2.440200</td>\n",
       "      <td>2.365421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1431</td>\n",
       "      <td>2.360300</td>\n",
       "      <td>2.363810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>2.366324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>2.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>2.550600</td>\n",
       "      <td>2.371265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>2.328600</td>\n",
       "      <td>2.364475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1476</td>\n",
       "      <td>2.442400</td>\n",
       "      <td>2.367649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>2.326700</td>\n",
       "      <td>2.359595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>2.542600</td>\n",
       "      <td>2.359907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1503</td>\n",
       "      <td>2.477600</td>\n",
       "      <td>2.364649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>2.442300</td>\n",
       "      <td>2.360193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>2.327700</td>\n",
       "      <td>2.358644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.470700</td>\n",
       "      <td>2.355770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1539</td>\n",
       "      <td>2.470700</td>\n",
       "      <td>2.358094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1548</td>\n",
       "      <td>2.194800</td>\n",
       "      <td>2.367459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1557</td>\n",
       "      <td>2.482600</td>\n",
       "      <td>2.358876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1566</td>\n",
       "      <td>2.369800</td>\n",
       "      <td>2.356262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.512100</td>\n",
       "      <td>2.357286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>2.385800</td>\n",
       "      <td>2.357839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1593</td>\n",
       "      <td>2.192700</td>\n",
       "      <td>2.362838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1602</td>\n",
       "      <td>2.168600</td>\n",
       "      <td>2.368214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>2.082900</td>\n",
       "      <td>2.357186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.346600</td>\n",
       "      <td>2.355147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1629</td>\n",
       "      <td>2.346600</td>\n",
       "      <td>2.353155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1638</td>\n",
       "      <td>2.475100</td>\n",
       "      <td>2.351709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1647</td>\n",
       "      <td>2.270900</td>\n",
       "      <td>2.356731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>2.393900</td>\n",
       "      <td>2.357597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>2.509500</td>\n",
       "      <td>2.352644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1674</td>\n",
       "      <td>2.411300</td>\n",
       "      <td>2.355176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1683</td>\n",
       "      <td>2.075600</td>\n",
       "      <td>2.353394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>2.349800</td>\n",
       "      <td>2.350875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1701</td>\n",
       "      <td>2.314500</td>\n",
       "      <td>2.357940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>2.353167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1719</td>\n",
       "      <td>2.340000</td>\n",
       "      <td>2.351421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>2.433900</td>\n",
       "      <td>2.350358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1737</td>\n",
       "      <td>2.352400</td>\n",
       "      <td>2.349915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1746</td>\n",
       "      <td>2.348100</td>\n",
       "      <td>2.352954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>2.510500</td>\n",
       "      <td>2.351214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1764</td>\n",
       "      <td>2.339300</td>\n",
       "      <td>2.348331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1773</td>\n",
       "      <td>2.299900</td>\n",
       "      <td>2.349103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1782</td>\n",
       "      <td>2.319200</td>\n",
       "      <td>2.346070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1791</td>\n",
       "      <td>2.445300</td>\n",
       "      <td>2.349984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.212700</td>\n",
       "      <td>2.350470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>2.212700</td>\n",
       "      <td>2.344338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>2.479200</td>\n",
       "      <td>2.345438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1827</td>\n",
       "      <td>2.292600</td>\n",
       "      <td>2.341164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1836</td>\n",
       "      <td>2.308100</td>\n",
       "      <td>2.341445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1845</td>\n",
       "      <td>2.022700</td>\n",
       "      <td>2.350564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1854</td>\n",
       "      <td>2.382800</td>\n",
       "      <td>2.347555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1863</td>\n",
       "      <td>2.393500</td>\n",
       "      <td>2.344038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1872</td>\n",
       "      <td>2.334900</td>\n",
       "      <td>2.344339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1881</td>\n",
       "      <td>2.146000</td>\n",
       "      <td>2.339909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>2.494800</td>\n",
       "      <td>2.341621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1899</td>\n",
       "      <td>2.494800</td>\n",
       "      <td>2.346121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1908</td>\n",
       "      <td>2.320000</td>\n",
       "      <td>2.342441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1917</td>\n",
       "      <td>2.349100</td>\n",
       "      <td>2.340919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1926</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>2.344918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1935</td>\n",
       "      <td>2.292800</td>\n",
       "      <td>2.338698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1944</td>\n",
       "      <td>2.474100</td>\n",
       "      <td>2.341096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1953</td>\n",
       "      <td>2.555800</td>\n",
       "      <td>2.341631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1962</td>\n",
       "      <td>2.097300</td>\n",
       "      <td>2.336584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1971</td>\n",
       "      <td>2.140900</td>\n",
       "      <td>2.336268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.102800</td>\n",
       "      <td>2.338461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1989</td>\n",
       "      <td>2.102800</td>\n",
       "      <td>2.339121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1998</td>\n",
       "      <td>2.321700</td>\n",
       "      <td>2.342674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2007</td>\n",
       "      <td>2.153100</td>\n",
       "      <td>2.341344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016</td>\n",
       "      <td>2.459900</td>\n",
       "      <td>2.340892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.372000</td>\n",
       "      <td>2.338811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2034</td>\n",
       "      <td>2.197800</td>\n",
       "      <td>2.334835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2043</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.334042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2052</td>\n",
       "      <td>2.353700</td>\n",
       "      <td>2.343034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2061</td>\n",
       "      <td>2.442100</td>\n",
       "      <td>2.339806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>2.162300</td>\n",
       "      <td>2.336152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2079</td>\n",
       "      <td>2.162300</td>\n",
       "      <td>2.331989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2088</td>\n",
       "      <td>2.448000</td>\n",
       "      <td>2.334617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2097</td>\n",
       "      <td>2.508100</td>\n",
       "      <td>2.336349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2106</td>\n",
       "      <td>2.421700</td>\n",
       "      <td>2.335366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2115</td>\n",
       "      <td>2.464100</td>\n",
       "      <td>2.330714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2124</td>\n",
       "      <td>2.173500</td>\n",
       "      <td>2.327562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2133</td>\n",
       "      <td>2.345000</td>\n",
       "      <td>2.326576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2142</td>\n",
       "      <td>2.144900</td>\n",
       "      <td>2.327173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2151</td>\n",
       "      <td>2.236400</td>\n",
       "      <td>2.329029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>2.190800</td>\n",
       "      <td>2.327136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2169</td>\n",
       "      <td>2.190800</td>\n",
       "      <td>2.325835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2178</td>\n",
       "      <td>2.505800</td>\n",
       "      <td>2.327953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2187</td>\n",
       "      <td>2.502400</td>\n",
       "      <td>2.327385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2196</td>\n",
       "      <td>2.080700</td>\n",
       "      <td>2.331522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2205</td>\n",
       "      <td>2.496300</td>\n",
       "      <td>2.329600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2214</td>\n",
       "      <td>2.376000</td>\n",
       "      <td>2.330041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2223</td>\n",
       "      <td>2.280300</td>\n",
       "      <td>2.326288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2232</td>\n",
       "      <td>2.194400</td>\n",
       "      <td>2.327984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2241</td>\n",
       "      <td>2.221600</td>\n",
       "      <td>2.326322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.363600</td>\n",
       "      <td>2.328510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2259</td>\n",
       "      <td>2.363600</td>\n",
       "      <td>2.326669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2268</td>\n",
       "      <td>2.291100</td>\n",
       "      <td>2.323261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2277</td>\n",
       "      <td>2.171500</td>\n",
       "      <td>2.322919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2286</td>\n",
       "      <td>2.340300</td>\n",
       "      <td>2.325857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2295</td>\n",
       "      <td>1.915400</td>\n",
       "      <td>2.327290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>2.170300</td>\n",
       "      <td>2.327877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2313</td>\n",
       "      <td>2.300600</td>\n",
       "      <td>2.324966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2322</td>\n",
       "      <td>2.212400</td>\n",
       "      <td>2.322767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2331</td>\n",
       "      <td>2.452400</td>\n",
       "      <td>2.323689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>2.312100</td>\n",
       "      <td>2.321440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2349</td>\n",
       "      <td>2.312100</td>\n",
       "      <td>2.321953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2358</td>\n",
       "      <td>2.424800</td>\n",
       "      <td>2.323695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2367</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>2.321931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2376</td>\n",
       "      <td>2.336700</td>\n",
       "      <td>2.321479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2385</td>\n",
       "      <td>2.248100</td>\n",
       "      <td>2.319325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2394</td>\n",
       "      <td>2.207500</td>\n",
       "      <td>2.319795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2403</td>\n",
       "      <td>2.341500</td>\n",
       "      <td>2.323732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2412</td>\n",
       "      <td>2.208900</td>\n",
       "      <td>2.318400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2421</td>\n",
       "      <td>2.061700</td>\n",
       "      <td>2.317430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>2.385300</td>\n",
       "      <td>2.316929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2439</td>\n",
       "      <td>2.385300</td>\n",
       "      <td>2.317328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2448</td>\n",
       "      <td>2.095100</td>\n",
       "      <td>2.316241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2457</td>\n",
       "      <td>2.146500</td>\n",
       "      <td>2.320249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2466</td>\n",
       "      <td>2.704600</td>\n",
       "      <td>2.317176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>2.324400</td>\n",
       "      <td>2.316476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2484</td>\n",
       "      <td>2.187100</td>\n",
       "      <td>2.315140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2493</td>\n",
       "      <td>2.348800</td>\n",
       "      <td>2.315651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2502</td>\n",
       "      <td>2.284300</td>\n",
       "      <td>2.318937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2511</td>\n",
       "      <td>2.225300</td>\n",
       "      <td>2.316148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>2.310900</td>\n",
       "      <td>2.313097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2529</td>\n",
       "      <td>2.310900</td>\n",
       "      <td>2.314009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2538</td>\n",
       "      <td>2.439200</td>\n",
       "      <td>2.314437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2547</td>\n",
       "      <td>2.304700</td>\n",
       "      <td>2.313638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2556</td>\n",
       "      <td>2.616200</td>\n",
       "      <td>2.314626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2565</td>\n",
       "      <td>2.239200</td>\n",
       "      <td>2.312278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2574</td>\n",
       "      <td>2.428100</td>\n",
       "      <td>2.311862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2583</td>\n",
       "      <td>2.323300</td>\n",
       "      <td>2.310772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2592</td>\n",
       "      <td>2.433100</td>\n",
       "      <td>2.312369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2601</td>\n",
       "      <td>2.174300</td>\n",
       "      <td>2.314466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2610</td>\n",
       "      <td>2.334700</td>\n",
       "      <td>2.309904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2619</td>\n",
       "      <td>2.334700</td>\n",
       "      <td>2.307360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2628</td>\n",
       "      <td>2.458700</td>\n",
       "      <td>2.308406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2637</td>\n",
       "      <td>2.121400</td>\n",
       "      <td>2.308437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2646</td>\n",
       "      <td>2.251000</td>\n",
       "      <td>2.308928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2655</td>\n",
       "      <td>2.494200</td>\n",
       "      <td>2.310951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2664</td>\n",
       "      <td>2.350600</td>\n",
       "      <td>2.307491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2673</td>\n",
       "      <td>2.091500</td>\n",
       "      <td>2.305976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2682</td>\n",
       "      <td>2.421000</td>\n",
       "      <td>2.304406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2691</td>\n",
       "      <td>2.362400</td>\n",
       "      <td>2.304820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.288900</td>\n",
       "      <td>2.308361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2709</td>\n",
       "      <td>2.288900</td>\n",
       "      <td>2.306325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2718</td>\n",
       "      <td>2.437600</td>\n",
       "      <td>2.305716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2727</td>\n",
       "      <td>2.030300</td>\n",
       "      <td>2.304247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2736</td>\n",
       "      <td>2.310400</td>\n",
       "      <td>2.302686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2745</td>\n",
       "      <td>1.977300</td>\n",
       "      <td>2.302148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2754</td>\n",
       "      <td>2.510900</td>\n",
       "      <td>2.302712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2763</td>\n",
       "      <td>2.428700</td>\n",
       "      <td>2.302393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2772</td>\n",
       "      <td>2.291900</td>\n",
       "      <td>2.302933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2781</td>\n",
       "      <td>2.136700</td>\n",
       "      <td>2.303973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2790</td>\n",
       "      <td>2.314500</td>\n",
       "      <td>2.303633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2799</td>\n",
       "      <td>2.314500</td>\n",
       "      <td>2.302830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2808</td>\n",
       "      <td>2.507500</td>\n",
       "      <td>2.301743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2817</td>\n",
       "      <td>2.263000</td>\n",
       "      <td>2.300679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2826</td>\n",
       "      <td>2.245000</td>\n",
       "      <td>2.301120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2835</td>\n",
       "      <td>2.487000</td>\n",
       "      <td>2.300911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2844</td>\n",
       "      <td>2.407900</td>\n",
       "      <td>2.301517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2853</td>\n",
       "      <td>2.467000</td>\n",
       "      <td>2.301925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2862</td>\n",
       "      <td>2.402500</td>\n",
       "      <td>2.300751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2871</td>\n",
       "      <td>2.344800</td>\n",
       "      <td>2.299930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>2.298947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2889</td>\n",
       "      <td>2.149000</td>\n",
       "      <td>2.297702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2898</td>\n",
       "      <td>2.388400</td>\n",
       "      <td>2.298422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2907</td>\n",
       "      <td>2.499000</td>\n",
       "      <td>2.298093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2916</td>\n",
       "      <td>2.079500</td>\n",
       "      <td>2.296279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>2.131700</td>\n",
       "      <td>2.296153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2934</td>\n",
       "      <td>2.200200</td>\n",
       "      <td>2.297247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2943</td>\n",
       "      <td>2.090900</td>\n",
       "      <td>2.298125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2952</td>\n",
       "      <td>2.245800</td>\n",
       "      <td>2.299562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2961</td>\n",
       "      <td>2.221700</td>\n",
       "      <td>2.296432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2970</td>\n",
       "      <td>2.013300</td>\n",
       "      <td>2.294388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2979</td>\n",
       "      <td>2.013300</td>\n",
       "      <td>2.293753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2988</td>\n",
       "      <td>2.194300</td>\n",
       "      <td>2.292691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2997</td>\n",
       "      <td>2.365500</td>\n",
       "      <td>2.292795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3006</td>\n",
       "      <td>2.180500</td>\n",
       "      <td>2.293714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3015</td>\n",
       "      <td>2.244500</td>\n",
       "      <td>2.291945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3024</td>\n",
       "      <td>2.159900</td>\n",
       "      <td>2.291796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3033</td>\n",
       "      <td>2.192600</td>\n",
       "      <td>2.292392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3042</td>\n",
       "      <td>2.618300</td>\n",
       "      <td>2.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3051</td>\n",
       "      <td>2.034800</td>\n",
       "      <td>2.293801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>2.264900</td>\n",
       "      <td>2.292659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3069</td>\n",
       "      <td>2.264900</td>\n",
       "      <td>2.290825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3078</td>\n",
       "      <td>2.137600</td>\n",
       "      <td>2.291567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3087</td>\n",
       "      <td>2.224200</td>\n",
       "      <td>2.291701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3096</td>\n",
       "      <td>2.301500</td>\n",
       "      <td>2.292545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3105</td>\n",
       "      <td>2.389500</td>\n",
       "      <td>2.292612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3114</td>\n",
       "      <td>1.795400</td>\n",
       "      <td>2.292470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3123</td>\n",
       "      <td>2.302800</td>\n",
       "      <td>2.291283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3132</td>\n",
       "      <td>2.376200</td>\n",
       "      <td>2.290490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3141</td>\n",
       "      <td>2.399300</td>\n",
       "      <td>2.290048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.358500</td>\n",
       "      <td>2.290791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3159</td>\n",
       "      <td>2.358500</td>\n",
       "      <td>2.288653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3168</td>\n",
       "      <td>2.170500</td>\n",
       "      <td>2.287288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3177</td>\n",
       "      <td>2.248700</td>\n",
       "      <td>2.287873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3186</td>\n",
       "      <td>2.270000</td>\n",
       "      <td>2.288430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3195</td>\n",
       "      <td>2.286500</td>\n",
       "      <td>2.287334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3204</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.287458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3213</td>\n",
       "      <td>2.125300</td>\n",
       "      <td>2.286660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3222</td>\n",
       "      <td>2.184500</td>\n",
       "      <td>2.286826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3231</td>\n",
       "      <td>2.467800</td>\n",
       "      <td>2.286876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>1.853700</td>\n",
       "      <td>2.284652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3249</td>\n",
       "      <td>1.853700</td>\n",
       "      <td>2.284098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3258</td>\n",
       "      <td>2.027500</td>\n",
       "      <td>2.284650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3267</td>\n",
       "      <td>2.061000</td>\n",
       "      <td>2.285495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3276</td>\n",
       "      <td>2.073200</td>\n",
       "      <td>2.285026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3285</td>\n",
       "      <td>2.179400</td>\n",
       "      <td>2.284048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3294</td>\n",
       "      <td>2.088500</td>\n",
       "      <td>2.283606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3303</td>\n",
       "      <td>1.986000</td>\n",
       "      <td>2.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3312</td>\n",
       "      <td>2.429800</td>\n",
       "      <td>2.283855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3321</td>\n",
       "      <td>2.318700</td>\n",
       "      <td>2.283703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3330</td>\n",
       "      <td>2.206700</td>\n",
       "      <td>2.282874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3339</td>\n",
       "      <td>2.206700</td>\n",
       "      <td>2.282014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3348</td>\n",
       "      <td>2.320600</td>\n",
       "      <td>2.281377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3357</td>\n",
       "      <td>2.100600</td>\n",
       "      <td>2.281632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3366</td>\n",
       "      <td>2.289900</td>\n",
       "      <td>2.281112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3375</td>\n",
       "      <td>2.141200</td>\n",
       "      <td>2.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3384</td>\n",
       "      <td>2.210600</td>\n",
       "      <td>2.280652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3393</td>\n",
       "      <td>2.336800</td>\n",
       "      <td>2.279707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3402</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>2.280062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3411</td>\n",
       "      <td>2.466600</td>\n",
       "      <td>2.279499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>2.303500</td>\n",
       "      <td>2.278088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3429</td>\n",
       "      <td>2.303500</td>\n",
       "      <td>2.278615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3438</td>\n",
       "      <td>2.258900</td>\n",
       "      <td>2.278817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3447</td>\n",
       "      <td>2.255000</td>\n",
       "      <td>2.278944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3456</td>\n",
       "      <td>2.167500</td>\n",
       "      <td>2.278999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3465</td>\n",
       "      <td>2.178100</td>\n",
       "      <td>2.279559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3474</td>\n",
       "      <td>2.313600</td>\n",
       "      <td>2.279636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3483</td>\n",
       "      <td>2.276900</td>\n",
       "      <td>2.279129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3492</td>\n",
       "      <td>2.202100</td>\n",
       "      <td>2.278277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3501</td>\n",
       "      <td>2.480300</td>\n",
       "      <td>2.277730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3510</td>\n",
       "      <td>2.412700</td>\n",
       "      <td>2.277629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3519</td>\n",
       "      <td>2.412700</td>\n",
       "      <td>2.276768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3528</td>\n",
       "      <td>2.103600</td>\n",
       "      <td>2.276757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3537</td>\n",
       "      <td>2.304800</td>\n",
       "      <td>2.276574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3546</td>\n",
       "      <td>2.156600</td>\n",
       "      <td>2.276652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3555</td>\n",
       "      <td>2.454700</td>\n",
       "      <td>2.277715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3564</td>\n",
       "      <td>2.351500</td>\n",
       "      <td>2.276594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3573</td>\n",
       "      <td>2.294200</td>\n",
       "      <td>2.275903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3582</td>\n",
       "      <td>2.128100</td>\n",
       "      <td>2.275350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3591</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>2.274910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.376100</td>\n",
       "      <td>2.274522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3609</td>\n",
       "      <td>2.376100</td>\n",
       "      <td>2.274610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3618</td>\n",
       "      <td>1.928600</td>\n",
       "      <td>2.273786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3627</td>\n",
       "      <td>2.253400</td>\n",
       "      <td>2.273939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3636</td>\n",
       "      <td>2.297800</td>\n",
       "      <td>2.274552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3645</td>\n",
       "      <td>2.353300</td>\n",
       "      <td>2.274024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3654</td>\n",
       "      <td>2.447200</td>\n",
       "      <td>2.273786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3663</td>\n",
       "      <td>2.211600</td>\n",
       "      <td>2.273062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3672</td>\n",
       "      <td>2.311100</td>\n",
       "      <td>2.272920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3681</td>\n",
       "      <td>1.969000</td>\n",
       "      <td>2.272505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3690</td>\n",
       "      <td>2.163900</td>\n",
       "      <td>2.272565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3699</td>\n",
       "      <td>2.163900</td>\n",
       "      <td>2.272170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3708</td>\n",
       "      <td>2.378500</td>\n",
       "      <td>2.271925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3717</td>\n",
       "      <td>2.011600</td>\n",
       "      <td>2.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3726</td>\n",
       "      <td>2.062100</td>\n",
       "      <td>2.271102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3735</td>\n",
       "      <td>2.110200</td>\n",
       "      <td>2.271110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3744</td>\n",
       "      <td>2.480200</td>\n",
       "      <td>2.271524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3753</td>\n",
       "      <td>2.386500</td>\n",
       "      <td>2.271807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3762</td>\n",
       "      <td>2.268500</td>\n",
       "      <td>2.271946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3771</td>\n",
       "      <td>2.147000</td>\n",
       "      <td>2.271533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>2.312600</td>\n",
       "      <td>2.271340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3789</td>\n",
       "      <td>2.312600</td>\n",
       "      <td>2.270430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3798</td>\n",
       "      <td>2.231700</td>\n",
       "      <td>2.269870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3807</td>\n",
       "      <td>2.355300</td>\n",
       "      <td>2.270105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3816</td>\n",
       "      <td>2.261400</td>\n",
       "      <td>2.269288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3825</td>\n",
       "      <td>2.096300</td>\n",
       "      <td>2.268892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3834</td>\n",
       "      <td>2.100400</td>\n",
       "      <td>2.268800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3843</td>\n",
       "      <td>1.574400</td>\n",
       "      <td>2.268747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3852</td>\n",
       "      <td>2.351000</td>\n",
       "      <td>2.268322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3861</td>\n",
       "      <td>2.356100</td>\n",
       "      <td>2.267418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3870</td>\n",
       "      <td>1.949700</td>\n",
       "      <td>2.267058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3879</td>\n",
       "      <td>1.949700</td>\n",
       "      <td>2.267364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3888</td>\n",
       "      <td>2.060600</td>\n",
       "      <td>2.267651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3897</td>\n",
       "      <td>2.187200</td>\n",
       "      <td>2.267388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3906</td>\n",
       "      <td>2.301800</td>\n",
       "      <td>2.267145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3915</td>\n",
       "      <td>2.303100</td>\n",
       "      <td>2.266880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3924</td>\n",
       "      <td>2.018400</td>\n",
       "      <td>2.266396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3933</td>\n",
       "      <td>2.081100</td>\n",
       "      <td>2.266167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3942</td>\n",
       "      <td>2.125800</td>\n",
       "      <td>2.266163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3951</td>\n",
       "      <td>2.113200</td>\n",
       "      <td>2.265959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>2.444600</td>\n",
       "      <td>2.265837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3969</td>\n",
       "      <td>2.444600</td>\n",
       "      <td>2.265481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3978</td>\n",
       "      <td>2.122100</td>\n",
       "      <td>2.265338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3987</td>\n",
       "      <td>2.300700</td>\n",
       "      <td>2.264891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3996</td>\n",
       "      <td>2.161000</td>\n",
       "      <td>2.264492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4005</td>\n",
       "      <td>2.437700</td>\n",
       "      <td>2.264213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4014</td>\n",
       "      <td>2.046200</td>\n",
       "      <td>2.264108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4023</td>\n",
       "      <td>2.338000</td>\n",
       "      <td>2.263851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4032</td>\n",
       "      <td>2.192300</td>\n",
       "      <td>2.263519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4041</td>\n",
       "      <td>2.366900</td>\n",
       "      <td>2.263360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.458600</td>\n",
       "      <td>2.263149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4059</td>\n",
       "      <td>2.458600</td>\n",
       "      <td>2.262980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4068</td>\n",
       "      <td>2.357400</td>\n",
       "      <td>2.262381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4077</td>\n",
       "      <td>2.030200</td>\n",
       "      <td>2.262053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4086</td>\n",
       "      <td>2.343600</td>\n",
       "      <td>2.261718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4095</td>\n",
       "      <td>2.265400</td>\n",
       "      <td>2.261748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4104</td>\n",
       "      <td>2.378700</td>\n",
       "      <td>2.261689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4113</td>\n",
       "      <td>1.940500</td>\n",
       "      <td>2.261513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4122</td>\n",
       "      <td>2.213600</td>\n",
       "      <td>2.261435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4131</td>\n",
       "      <td>2.388400</td>\n",
       "      <td>2.261369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>2.263200</td>\n",
       "      <td>2.261272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4149</td>\n",
       "      <td>2.263200</td>\n",
       "      <td>2.261147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4158</td>\n",
       "      <td>2.371700</td>\n",
       "      <td>2.261092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4167</td>\n",
       "      <td>2.101100</td>\n",
       "      <td>2.260736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4176</td>\n",
       "      <td>2.274400</td>\n",
       "      <td>2.260382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4185</td>\n",
       "      <td>2.460400</td>\n",
       "      <td>2.260066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4194</td>\n",
       "      <td>2.237000</td>\n",
       "      <td>2.259856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4203</td>\n",
       "      <td>2.451400</td>\n",
       "      <td>2.259693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4212</td>\n",
       "      <td>2.249600</td>\n",
       "      <td>2.259631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4221</td>\n",
       "      <td>2.048800</td>\n",
       "      <td>2.259596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4230</td>\n",
       "      <td>2.200200</td>\n",
       "      <td>2.259449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4239</td>\n",
       "      <td>2.200200</td>\n",
       "      <td>2.259222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4248</td>\n",
       "      <td>2.071500</td>\n",
       "      <td>2.259133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4257</td>\n",
       "      <td>2.521200</td>\n",
       "      <td>2.259109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4266</td>\n",
       "      <td>2.283800</td>\n",
       "      <td>2.259073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4275</td>\n",
       "      <td>2.216100</td>\n",
       "      <td>2.259091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4284</td>\n",
       "      <td>2.356800</td>\n",
       "      <td>2.259102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4293</td>\n",
       "      <td>2.249100</td>\n",
       "      <td>2.258916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4302</td>\n",
       "      <td>2.314800</td>\n",
       "      <td>2.258684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4311</td>\n",
       "      <td>2.151400</td>\n",
       "      <td>2.258526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>2.336900</td>\n",
       "      <td>2.258232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4329</td>\n",
       "      <td>2.336900</td>\n",
       "      <td>2.258050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4338</td>\n",
       "      <td>2.150500</td>\n",
       "      <td>2.258013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4347</td>\n",
       "      <td>2.198400</td>\n",
       "      <td>2.258046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4356</td>\n",
       "      <td>2.385100</td>\n",
       "      <td>2.258042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4365</td>\n",
       "      <td>2.286500</td>\n",
       "      <td>2.257997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4374</td>\n",
       "      <td>2.278600</td>\n",
       "      <td>2.257943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4383</td>\n",
       "      <td>2.407000</td>\n",
       "      <td>2.257817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4392</td>\n",
       "      <td>2.141100</td>\n",
       "      <td>2.257633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4401</td>\n",
       "      <td>2.418400</td>\n",
       "      <td>2.257511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4410</td>\n",
       "      <td>2.141200</td>\n",
       "      <td>2.257469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4419</td>\n",
       "      <td>2.141200</td>\n",
       "      <td>2.257358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4428</td>\n",
       "      <td>2.068200</td>\n",
       "      <td>2.257276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4437</td>\n",
       "      <td>2.355800</td>\n",
       "      <td>2.257221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4446</td>\n",
       "      <td>2.436300</td>\n",
       "      <td>2.257174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4455</td>\n",
       "      <td>2.207800</td>\n",
       "      <td>2.257122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4464</td>\n",
       "      <td>1.807200</td>\n",
       "      <td>2.257101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4473</td>\n",
       "      <td>2.377200</td>\n",
       "      <td>2.257093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4482</td>\n",
       "      <td>2.157200</td>\n",
       "      <td>2.257077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4491</td>\n",
       "      <td>1.985100</td>\n",
       "      <td>2.257064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.431900</td>\n",
       "      <td>2.257072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/home/vincent/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4500, training_loss=2.3298106002807617, metrics={'train_runtime': 35937.7894, 'train_samples_per_second': 0.25, 'train_steps_per_second': 0.125, 'total_flos': 9.290029150303027e+16, 'train_loss': 2.3298106002807617, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab36a50-ad47-4942-a8dc-8c6cf38a1d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 6.8%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–‡â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–â–‚â–â–ˆâ–â–â–â–…â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–ˆâ–â–â–â–â–â–</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–‡â–†â–‡â–â–ˆâ–ˆâ–ˆâ–ƒâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–‡â–†â–‡â–â–ˆâ–ˆâ–ˆâ–ƒâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ƒâ–‚â–„â–‚â–„â–ˆâ–ƒâ–…â–‚â–ƒâ–‚â–‚â–†â–ƒâ–†â–‚â–‚â–‚â–ƒâ–„â–ƒâ–…â–‚â–ƒâ–â–ƒâ–ƒâ–„â–‡â–â–ƒâ–â–ƒâ–‚â–ƒâ–…â–‚â–‚â–‚â–‚</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–‡â–‡â–‡â–‡â–…â–ƒâ–‡â–ˆâ–â–†â–„â–…â–ˆâ–…â–ƒâ–†â–‚â–‚â–†â–†â–…â–…â–„â–„â–†â–ƒâ–ƒâ–„â–…â–„â–„â–…â–â–„â–‚â–†â–ƒâ–‚â–„â–ƒ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.25707</td></tr><tr><td>eval/runtime</td><td>65.1216</td></tr><tr><td>eval/samples_per_second</td><td>15.356</td></tr><tr><td>eval/steps_per_second</td><td>15.356</td></tr><tr><td>total_flos</td><td>9.290029150303027e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>4500</td></tr><tr><td>train/grad_norm</td><td>1.50388</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.4319</td></tr><tr><td>train_loss</td><td>2.32981</td></tr><tr><td>train_runtime</td><td>35937.7894</td></tr><tr><td>train_samples_per_second</td><td>0.25</td></tr><tr><td>train_steps_per_second</td><td>0.125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ancient-meadow-9</strong> at: <a href='https://wandb.ai/raisepartner/llama3-fine-tune-tutorial/runs/17p3dotz' target=\"_blank\">https://wandb.ai/raisepartner/llama3-fine-tune-tutorial/runs/17p3dotz</a><br/> View project at: <a href='https://wandb.ai/raisepartner/llama3-fine-tune-tutorial' target=\"_blank\">https://wandb.ai/raisepartner/llama3-fine-tune-tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240628_115657-17p3dotz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fb611d-0905-4cea-98e4-29b69a0818c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello doctor, I have bad acne. How do I get rid of it?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m      9\u001b[0m                                        add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, \n\u001b[1;32m     15\u001b[0m                          num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:803\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:803\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    805\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, \n",
    "                   truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150, \n",
    "                         num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ef2e4a9-c485-4d9f-a492-6c8d65b0acd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpush_to_hub(new_model, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/peft_model.py:251\u001b[0m, in \u001b[0;36mPeftModel.save_pretrained\u001b[0;34m(self, save_directory, safe_serialization, selected_adapters, save_embedding_layers, is_main_process, convert_pissa_to_lora, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# save only the trainable weights\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m output_state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_embedding_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_embedding_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, adapter_name) \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m save_directory\n\u001b[1;32m    258\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:73\u001b[0m, in \u001b[0;36mget_peft_model_state_dict\u001b[0;34m(model, state_dict, adapter_name, unwrap_compiled, save_embedding_layers)\u001b[0m\n\u001b[1;32m     71\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;129;01min\u001b[39;00m (PeftType\u001b[38;5;241m.\u001b[39mLORA, PeftType\u001b[38;5;241m.\u001b[39mADALORA):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# to_return = lora_state_dict(model, bias=model.peft_config.bias)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# adapted from `https://github.com/microsoft/LoRA/blob/main/loralib/utils.py`\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# to be used directly with the state dict which is necessary when using DeepSpeed or FSDP\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     bias \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1916\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[0;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1918\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1916\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[0;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1918\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "    \u001b[0;31m[... skipping similar frames: Module.state_dict at line 1916 (5 times)]\u001b[0m\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1916\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[0;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1918\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1913\u001b[0m, in \u001b[0;36mModule.state_dict\u001b[0;34m(self, destination, prefix, keep_vars, *args)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state_dict_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1912\u001b[0m     hook(\u001b[38;5;28mself\u001b[39m, prefix, keep_vars)\n\u001b[0;32m-> 1913\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_to_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:439\u001b[0m, in \u001b[0;36mLinear4bit._save_to_state_dict\u001b[0;34m(self, destination, prefix, keep_vars)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_save_to_state_dict(destination, prefix, keep_vars)  \u001b[38;5;66;03m# saving weight and bias\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_state\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    440\u001b[0m         destination[prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m k] \u001b[38;5;241m=\u001b[39m v \u001b[38;5;28;01mif\u001b[39;00m keep_vars \u001b[38;5;28;01melse\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/dev/ai/tiny-llama3/.venv/lib/python3.11/site-packages/bitsandbytes/functional.py:751\u001b[0m, in \u001b[0;36mQuantState.as_dict\u001b[0;34m(self, packed)\u001b[0m\n\u001b[1;32m    738\u001b[0m qs_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquant_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_type,\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mabsmax,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape),\n\u001b[1;32m    745\u001b[0m }\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnested:\n\u001b[1;32m    747\u001b[0m     qs_dict\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    748\u001b[0m         {\n\u001b[1;32m    749\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested_absmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate2\u001b[38;5;241m.\u001b[39mabsmax,\n\u001b[1;32m    750\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested_blocksize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate2\u001b[38;5;241m.\u001b[39mblocksize,\n\u001b[0;32m--> 751\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested_quant_map\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# un-shared to avoid restoring it after shared tensors are removed by safetensors\u001b[39;00m\n\u001b[1;32m    752\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate2\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    753\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    754\u001b[0m         },\n\u001b[1;32m    755\u001b[0m     )\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packed:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qs_dict\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7459e5a-ed58-4373-b8d8-c0a8a72c283b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
